{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "James Quirk (james.f.quirk.25@dartmouth.edu) / CS 72 Final Project / 03/01/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary packages\n",
    "import pandas as pd\n",
    "import autograd.numpy as np\n",
    "import statistics\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15347, 0.281, 0.68, 0.467]\n"
     ]
    }
   ],
   "source": [
    "# Store the number of appearances of each word as they are found in the VAD lexicon\n",
    "def create_dict():\n",
    "    filename = \"NRC-VAD-Lexicon.txt\"\n",
    "    VAD = {}\n",
    "    i = 0\n",
    "    with open(filename, 'r') as file:\n",
    "        line = file.readline()\n",
    "        \n",
    "        # Iterate through every line\n",
    "        while line:\n",
    "            info = line.strip().split('\\t')            \n",
    "            word = info[0]\n",
    "            \n",
    "            # Ignores words that do not have an associated embedding\n",
    "            if word not in VAD:\n",
    "                VAD[word] = [i, float(info[1]), float(info[2]), float(info[3])]\n",
    "                i += 1\n",
    "            line = file.readline()\n",
    "\n",
    "    return VAD\n",
    "\n",
    "VAD = create_dict()\n",
    "\n",
    "# Ensure that information loaded correctly\n",
    "print(VAD[\"saturate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that maps classes to indices\n",
    "labels = {\"crime, law and justice\":1, \"arts, culture, entertainment and media\":2,\n",
    "          \"economy, business and finance\":3, \"disaster, accident and emergency incident\":4,\n",
    "          \"environment\":5, \"education\":6, \"health\":7, \"human interest\":8, \"lifestyle and leisure\":9, \n",
    "          \"politics\":10, \"labour\":11, \"religion and belief\":12, \"science and technology\":13, \"society\":14, \n",
    "          \"sport\":15, \"conflict, war and peace\":16, \"weather\":17}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"MN-DS-news-classification.csv\")\n",
    "\n",
    "# Extract all documents and their associated classes (news topics)\n",
    "nX = data[\"content\"]\n",
    "ny = data[\"category_level_1\"]\n",
    "\n",
    "# Fill feature vectors according to number of words within them: bag of words implementation\n",
    "X = np.zeros((len(nX), len(VAD)))\n",
    "y = np.zeros(len(ny))\n",
    "for i, doc in enumerate(nX):\n",
    "    feature_vector = len(VAD)*[0]\n",
    "    y[i] = labels[ny[i]]\n",
    "    words = doc.lower().strip().split()\n",
    "\n",
    "    for word in words:\n",
    "        if word in VAD:\n",
    "            index = VAD[word][0]\n",
    "            X[i][index] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"18\">**k-NN without embeddings**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanded from my own implementation of k-NN from CS74 (written from scratch)\n",
    "\n",
    "# Pass in X_train and y_train to compare to every X_test value\n",
    "def knn_classifier(X_train, X_test):\n",
    "  close_array = [] # Array of predicted classes\n",
    "\n",
    "  # For every data point\n",
    "  for i, datapoint in enumerate(X_test):\n",
    "\n",
    "    # Create an array of the distance to each training point\n",
    "    dist_array = []\n",
    "    for j, comparison in enumerate(X_train):\n",
    "      distance = np.power(np.sum(np.power(np.abs(datapoint - comparison), 2)), 0.5) # Find euclidean distance between every test point and every train point\n",
    "      dist_array.append((j, distance)) # Store the index so that we can index into y_test to find the class and the distance to organize in ascending order\n",
    "\n",
    "    # Sort in ascending order\n",
    "    dist_array.sort(key=lambda dist: dist[1])\n",
    "\n",
    "    close_array.append(dist_array)\n",
    "\n",
    "  # Return the array of predicted classes\n",
    "  return close_array\n",
    "\n",
    "y_pred = knn_classifier(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanded from my own implementation of k-NN from CS74 (written from scratch)\n",
    "\n",
    "# Helper function that allows us to change the k value without having to re-run comparison between train and test\n",
    "def with_k(dist_array, k):\n",
    "    pred_class = []\n",
    "    for ele in dist_array:\n",
    "        # List of closest values' classes\n",
    "        pred = []\n",
    "        for within_k in range(k): # Iterate through however many neighbors specified\n",
    "            ind = ele[within_k][0] # Grab the index of the closest training points\n",
    "            pred.append(y_train[ind]) # Find the class of the closest training points and append them\n",
    "\n",
    "        pred_class.append(statistics.mode(pred)) # Finds the class with the most appearances and saves it as this test point's predicted class\n",
    "\n",
    "    return pred_class\n",
    "\n",
    "# Grab the accuracies for k = 1 through 6\n",
    "pred_1 = with_k(y_pred, 1)\n",
    "pred_2 = with_k(y_pred, 2)\n",
    "pred_3 = with_k(y_pred, 3)\n",
    "pred_4 = with_k(y_pred, 4)\n",
    "pred_5 = with_k(y_pred, 5)\n",
    "pred_6 = with_k(y_pred, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended from CS 74, I coded this from scratch\n",
    "\n",
    "# Metric to evaluate the success of the multi-class classifier\n",
    "def evaluate(y_actual,y_pred):\n",
    "    success = 0\n",
    "    for i in range(len(y_pred)):\n",
    "      # Cast characters as integers to compare accurately\n",
    "      if int(y_actual[i]) == int(y_pred[i]):\n",
    "        success += 1 # Whenever prediction is right, add one to the correct counter\n",
    "\n",
    "    accuracy = success/len(y_pred) # Accuracy = success/total\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1 accuracy: 0.43223443223443225\n",
      "k = 2 accuracy: 0.43223443223443225\n",
      "k = 3 accuracy: 0.4409340659340659\n",
      "k = 4 accuracy: 0.440018315018315\n",
      "k = 5 accuracy: 0.4409340659340659\n",
      "k = 6 accuracy: 0.43864468864468864\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation function\n",
    "accuracy = evaluate(y_test, pred_1)\n",
    "accuracy2 = evaluate(y_test, pred_2)\n",
    "accuracy3 = evaluate(y_test, pred_3)\n",
    "accuracy4 = evaluate(y_test, pred_4)\n",
    "accuracy5 = evaluate(y_test, pred_5)\n",
    "accuracy6 = evaluate(y_test, pred_6)\n",
    "\n",
    "# Print output\n",
    "print(\"k = 1 accuracy:\", accuracy)\n",
    "print(\"k = 2 accuracy:\", accuracy2)\n",
    "print(\"k = 3 accuracy:\", accuracy3)\n",
    "print(\"k = 4 accuracy:\", accuracy4)\n",
    "print(\"k = 5 accuracy:\", accuracy5)\n",
    "print(\"k = 6 accuracy:\", accuracy6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 38   2   2   9   0   2   0   9   1   9   9   2   2  10   2   6   0]\n",
      " [  1  24   1   1   0   2   1   5   2   3   3   2   0   4   4   2   0]\n",
      " [  1   0  25   3   0   2   4   8   0  14   5   0   2   5   3   2   0]\n",
      " [  1   0   1  44   4   4   2  13   1   4   1   1   1   5   2   3   9]\n",
      " [  0   0   4  12  46   0   4  18   3  13   0   1   8   4   3   5   2]\n",
      " [  0   0   1   3   0  66   8   6   0   5   3  10  10   4   5   3   1]\n",
      " [  2   1   0   8   3   4  71  16   1  15   5   1   6  12   3   1   3]\n",
      " [  0   2   3  13   4   3   5  42   1  15   4   2   5   9   7   2   0]\n",
      " [  0   2   0   4   0   0   0   7  23   0   2   1   5   3   4   0   2]\n",
      " [  8   4   4  11   2   2   5  20   1  84  10   7   6   8   3  10   0]\n",
      " [  1   1   4   9   0   5   1  12   0   6  78   4   2   6   4   2   3]\n",
      " [  3   3   1  10   1   5   1  23   2  18   1  65   3  10   4   6   3]\n",
      " [  0   2   2   6   5   4  15  15   2  20   8   4  49   9   4   5   1]\n",
      " [  5   3   5  29   2   3  16  25   4  12  17   9  11  73  12   1   0]\n",
      " [  2   1   2   6   1   3   5  16   4  14  12   1   4   4 104   1   4]\n",
      " [  4   0   2  15   1   1   5  14   2  25   9   5   2   4   2  74   0]\n",
      " [  1   1   0   9   0   0   0   3   0   5   0   0   0   0   0   1  57]]\n"
     ]
    }
   ],
   "source": [
    "# Import and create confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_matrix = confusion_matrix(y_test, pred_3)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"18\">**kNN with embeddings**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty vectors\n",
    "X = np.zeros((len(nX), 3))\n",
    "y = np.zeros(len(ny))\n",
    "\n",
    "# Then calculate mean V, A, and D scores for each document\n",
    "for i, doc in enumerate(nX):\n",
    "    valence = 0\n",
    "    arousal = 0\n",
    "    dominance = 0\n",
    "    count = 0\n",
    "\n",
    "    y[i] = labels[ny[i]]\n",
    "    words = doc.lower().strip().split()\n",
    "\n",
    "    # Apply to every word in every document that is in our dictionary\n",
    "    for word in words:\n",
    "        if word in VAD:\n",
    "            valence += VAD[word][1]\n",
    "            arousal += VAD[word][2]\n",
    "            dominance += VAD[word][3]\n",
    "            count += 1\n",
    "    \n",
    "    # Converts from sum to mean\n",
    "    if count != 0:\n",
    "        valence /= count\n",
    "        arousal /= count\n",
    "        dominance /= count\n",
    "\n",
    "    X[i][0] = valence\n",
    "    X[i][1] = arousal\n",
    "    X[i][2] = dominance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar function to the one found above\n",
    "\n",
    "# Pass in X_train and y_train to compare to every X_test value\n",
    "def knn_classifier_with_embeddings(X_train, X_test):\n",
    "  close_array = [] # Array of predicted classes\n",
    "\n",
    "  # For every data point\n",
    "  for i, datapoint in enumerate(X_test):\n",
    "\n",
    "    # Create an array of the distance to each training point\n",
    "    dist_array = []\n",
    "    for j, comparison in enumerate(X_train):\n",
    "      distance = np.power(np.sum(np.power(np.abs(datapoint - comparison), 2)), 0.5) # Find euclidean distance between every test point and every train point\n",
    "      dist_array.append((j, distance)) # Store the index so that we can index into y_test to find the class and the distance to organize in ascending order\n",
    "\n",
    "    # Sort in ascending order\n",
    "    dist_array.sort(key=lambda dist: dist[1])\n",
    "\n",
    "    close_array.append(dist_array)\n",
    "\n",
    "  # Return the array of predicted classes\n",
    "  return close_array\n",
    "\n",
    "# Split into training and testing\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "y_pred = knn_classifier_with_embeddings(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predictions for k = 1 through 5 & 100\n",
    "pred_1 = with_k(y_pred, 1)\n",
    "pred_2 = with_k(y_pred, 2)\n",
    "pred_3 = with_k(y_pred, 3)\n",
    "pred_4 = with_k(y_pred, 4)\n",
    "pred_5 = with_k(y_pred, 5)\n",
    "pred_100 = with_k(y_pred, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k = 1 accuracy: 0.16758241758241757\n",
      "k = 2 accuracy: 0.16758241758241757\n",
      "k = 3 accuracy: 0.17261904761904762\n",
      "k = 4 accuracy: 0.17261904761904762\n",
      "k = 5 accuracy: 0.184981684981685\n",
      "k = 100 accuracy: 0.2174908424908425\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation function\n",
    "accuracy = evaluate(y_test, pred_1)\n",
    "accuracy2 = evaluate(y_test, pred_2)\n",
    "accuracy3 = evaluate(y_test, pred_3)\n",
    "accuracy4 = evaluate(y_test, pred_4)\n",
    "accuracy5 = evaluate(y_test, pred_5)\n",
    "accuracy100 = evaluate(y_test, pred_100)\n",
    "\n",
    "# Print output\n",
    "print(\"k = 1 accuracy:\", accuracy)\n",
    "print(\"k = 2 accuracy:\", accuracy2)\n",
    "print(\"k = 3 accuracy:\", accuracy3)\n",
    "print(\"k = 4 accuracy:\", accuracy4)\n",
    "print(\"k = 5 accuracy:\", accuracy5)\n",
    "print(\"k = 100 accuracy:\", accuracy100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 4  0  0  5  3  0  3  0  0 24  1  1  6 10  3 32  2]\n",
      " [ 0  0  0  1  1 10  1  0  4  3  2  1  2 17 13  2  2]\n",
      " [ 0  0  0  2  6  4  6  1  0 20  1  4 11  1  6  9  5]\n",
      " [ 3  0  0 15  2  2 10  0  1  4  1  1  4 10 10 16 16]\n",
      " [ 1  0  0  7 22  5  6  0  1 19  0  6 17 10  4  6 12]\n",
      " [ 0  0  0  0  3 44  1  5  1 22  3 13 25  7 14  4  0]\n",
      " [ 5  0  1 17  7 10 12  0  2 15  5  6 16 13 15 16  7]\n",
      " [ 2  0  0  3  3 12  0 22 18  3  1  3  2 22 19  2  5]\n",
      " [ 0  0  0  0  1  2  0  1 18  0  0  0  0 12 27  0  2]\n",
      " [ 0  0  1  5  7  8  8  4  2 70  9 12 21 11  4 38  4]\n",
      " [ 2  0  0  6 12 15  5  6  1 18  3  7 19 14 21  5 10]\n",
      " [ 9  0  0  2  3 17  4  3  2 37  3 10 20 12  7 20  1]\n",
      " [ 1  0  0  0 15 23  9  8  1 26  2  6 29 11 11 10  5]\n",
      " [ 4  0  0  7  6 17  8  8  2 22  5  8 16 53 34 18  8]\n",
      " [ 3  0  0  5  4 10  5  7  2 10  2  7 11 15 70  9  6]\n",
      " [ 3  0  0  4  3  5  8  1  1 42  3  3  9  9  7 55  0]\n",
      " [ 3  0  0  9  5  0  7  0  2  0  0  0  2  9  1  3 44]]\n"
     ]
    }
   ],
   "source": [
    "conf_matrix = confusion_matrix(y_test, pred_100)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"18\">**SVM**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5146520146520146\n",
      "[[  5   3   0   0   0   0   0   0   2   1   0   5   3   3  44   2   0]\n",
      " [  0 116   3   2   0   0   0   0   1   0   0  10   3   3  23   0   0]\n",
      " [  0   4  42   5   0   1   0   1   1   1   0  12   0   1  33   0   0]\n",
      " [  1   6   2  36   1   0   2   0   0   0   0   8   0   1  28   1   7]\n",
      " [  0   4   0   0  20   0   1   0   0   6   0  18   0   8  25   2   0]\n",
      " [  0   0   1   1   0  62   0   0   0   0   0   2   6   7  32   0   0]\n",
      " [  0   2   0   2   1   0  75   1   0   0   1   4   3  11  24   1   0]\n",
      " [  0   2   0   2   0   1   0  58   0   1   1   5   0  21  45   0   0]\n",
      " [  0   5   0   2   0   1   3   1  20   0   5   0   3   6  65   7   0]\n",
      " [  0   2   3   0   0   2   0   1   0  64   1  11   0   7  36  13   0]\n",
      " [  0   0   0   0   0   0   0   1   1   0  25   1   2   4  26   5   0]\n",
      " [  0  17   1   0   1   0   2   2   0   2   0  86   1  13  53   0   0]\n",
      " [  0  16   2   2   0   0   0   0   1   0   1   9  86   4  58   0   0]\n",
      " [  0   1   2   1   0   4   3   3   2   0   1   4   1  86  42   2   0]\n",
      " [  0   7   2   2   0   4   2   1   1   3   3   8  10   8 151   3   0]\n",
      " [  0   0   0   0   0   1   0   0   1   0   9   1   2   3  33 132   0]\n",
      " [  0   0   0   9   0   0   1   0   0   0   0   1   0   0  15   0  60]]\n"
     ]
    }
   ],
   "source": [
    "# Extended from Lizzy's code on Logistic Regression\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#data organizing for sklearn model\n",
    "df = pd.read_csv('MN-DS-news-classification.csv')\n",
    "\n",
    "#clean\n",
    "df['content'] = df['content'].apply(lambda x: ''.join(char for char in x if char.isalnum() or char.isspace()))\n",
    "df['content'] = df['content'].map(lambda x: x.lower())\n",
    "\n",
    "#pull relevent columns\n",
    "articles = df['content'].tolist()\n",
    "topics = df['category_level_1'].tolist()\n",
    "\n",
    "#split to test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(articles, topics, test_size=0.2, random_state=42)\n",
    "\n",
    "# text to vector representation via bag of words\n",
    "vectorizer = CountVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train)\n",
    "X_test = vectorizer.transform(X_test)\n",
    "\n",
    "# log reg classifier\n",
    "lr = SVC(max_iter = 2000)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# go through test set\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "# get accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['society' 'health' 'society' 'society' 'weather' 'lifestyle and leisure'\n",
      " 'society' 'education' 'health' 'human interest' 'society' 'society'\n",
      " 'society' 'society' 'religion and belief' 'society' 'sport' 'labour'\n",
      " 'society' 'sport' 'politics' 'education' 'society' 'society' 'society'\n",
      " 'science and technology' 'conflict, war and peace' 'politics'\n",
      " 'economy, business and finance' 'conflict, war and peace']\n"
     ]
    }
   ],
   "source": [
    "# Visualize the frequency of classifying documents as \"society\"\n",
    "print(y_pred[0:30])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
